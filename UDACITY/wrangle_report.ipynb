{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Udacity\n",
    "\n",
    "> Create a 300-600 word written report called wrangle_report.pdf or wrangle_report.html that briefly describes your wrangling efforts. This is to be framed as an internal document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [David Capella](http://davidcapella.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The start of this project, which should be basically the same for any project, involves gathering data. The two important CSV was actually given to me by [Udacity](www.udacity.com). The third dataset I need to gather from the Tweet Ids. I put both Tweet Id columns from the two CSVs into a set (that way I would only have the unique ones). With the help of tweepy, I collected all the tweets from twitter of several different attributes: If it was retweeted, retweet count, favorite count, if it is the quote status, and if it was favorited. Now the only two I really needed were retweet count and favorite count, however, I grabbed more attributes because you can never have too much data (maybe you can?). Also, I needed to make sure these tweets were not retweeted from someone else.\n",
    "\n",
    "Armed with this information, I am now ready to go into data assessing. The first step in this process is to inspect the data visually. Visually inspecting is a long process that does not always yield a lot of results. That is O.K. because afterwards I will inspect the data again but with Python. From visually inspecting I know that I will have to look at the data types of certain columns. I also see a lot of 'Nulls' that I will have to inspect a little more. In addition, there are multiple columns for what type of dog it is; according to some dog chart. I can see that there are two URLs for some reason. After this, I can now inspect it programmatically. I can view info of the datasets and inspect each column using different tactics.\n",
    "\n",
    "After getting all the tidiness and quality issues written down as a list, I can now act upon it. Finally some data cleaning. First things first; create a copy of each data frame for cleaning. For each problem, I will define, code, and test it. It took a little bit, but it was well worth the effort. There was a couple of problems that I was not able to solve such as the wrong names; this is unfortunate but in the grand scheme of things, not very necessary. Also, going through this part showed some other problems that I had originally missed. This is O.K. because it is all part of the process. I would simply add it to my list of tidiness and quality issues and place it in the correct spot in the cleaning area. The final step to this part is to save the clean data sets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
